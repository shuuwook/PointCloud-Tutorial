{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "from module.dataset import ModelNet40\n",
    "from module.utils import *\n",
    "\n",
    "import os, sys\n",
    "from collections import OrderedDict\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class MaxPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaxPooling, self).__init__()\n",
    "\n",
    "    def forward(self, x, dim=1, keepdim = False):\n",
    "        res, _ = torch.max(x, dim=dim, keepdim = keepdim)\n",
    "        return res\n",
    "\n",
    "class Permute(nn.Module):\n",
    "    def __init__(self, param):\n",
    "        super(Permute, self).__init__()\n",
    "        self.param = param\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.permute(self.param)\n",
    "\n",
    "class BatchNorm(nn.Module):\n",
    "    '''\n",
    "        Perform batch normalization.\n",
    "        Input: A tensor of size (N, M, feature_dim), or (N, feature_dim, M) (available when feature_dim != M), \n",
    "                or (N, feature_dim)\n",
    "        Output: A tensor of the same size as input.\n",
    "    '''\n",
    "    def __init__(self, feature_dim):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.batchnorm = nn.BatchNorm1d(feature_dim)\n",
    "        self.permute = Permute((0, 2, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if (len(x.shape) == 3) and (x.shape[-1] == self.feature_dim):\n",
    "            return self.permute(self.batchnorm(self.permute(x)))\n",
    "        else:\n",
    "            return self.batchnorm(x)\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, batchnorm = True, last_activation = True):\n",
    "        super(MLP, self).__init__()\n",
    "        q = []\n",
    "        for i in range(len(hidden_size)-1):\n",
    "            in_dim = hidden_size[i]\n",
    "            out_dim = hidden_size[i+1]\n",
    "            q.append((\"Linear_%d\" % i, nn.Linear(in_dim, out_dim)))\n",
    "            if (i < len(hidden_size) - 2) or ((i == len(hidden_size) - 2) and (last_activation)):\n",
    "                if (batchnorm):\n",
    "                    q.append((\"Batchnorm_%d\" % i, BatchNorm(out_dim)))\n",
    "                q.append((\"ReLU_%d\" % i, nn.ReLU(inplace=True)))\n",
    "        self.mlp = nn.Sequential(OrderedDict(q))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNet(nn.Module):\n",
    "    def __init__(self, nfeat):\n",
    "        super(TNet, self).__init__()\n",
    "        self.nfeat = nfeat\n",
    "        self.tnet = nn.Sequential(MLP((nfeat, 64, 128, 1024)), MaxPooling(), \n",
    "                                  BatchNorm(1024), MLP((1024, 512, 256, nfeat*nfeat)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        return self.tnet(x).view(batch_size, self.nfeat, self.nfeat)\n",
    "    \n",
    "class PointNet(nn.Module):\n",
    "    def __init__(self, nfeat, nclass, dropout = 0):\n",
    "        super(PointNet, self).__init__()\n",
    "\n",
    "        self.input_transform = TNet(nfeat)\n",
    "        self.mlp1 = nn.Sequential(BatchNorm(3), MLP((nfeat, 64, 64)))\n",
    "        self.feature_transform = TNet(64)\n",
    "        self.mlp2 = nn.Sequential(BatchNorm(64), MLP((64, 64, 128, 1024)))\n",
    "        self.maxpooling = MaxPooling()\n",
    "        self.mlp3 = nn.Sequential(BatchNorm(1024), MLP((1024, 512, 256)), nn.Dropout(dropout), nn.Linear(256, nclass))\n",
    "        \n",
    "        self.eye64 = torch.eye(64).to(device)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        batch_size = xs.shape[0]\n",
    "        \n",
    "        transform = self.input_transform(xs)\n",
    "        xs = torch.stack([torch.mm(xs[i],transform[i]) for i in range(batch_size)])\n",
    "        xs = self.mlp1(xs)\n",
    "        \n",
    "        transform = self.feature_transform(xs)\n",
    "        xs = torch.stack([torch.mm(xs[i],transform[i]) for i in range(batch_size)])\n",
    "        xs = self.mlp2(xs)\n",
    "        \n",
    "        xs = self.mlp3(self.maxpooling(xs))\n",
    "        \n",
    "        if (self.training):\n",
    "            transform_transpose = transform.transpose(1, 2)\n",
    "            tmp = torch.stack([torch.mm(transform[i], transform_transpose[i]) for i in range(batch_size)])\n",
    "            L_reg = ((tmp - self.eye64) ** 2).sum() / batch_size\n",
    "            \n",
    "        return (F.log_softmax(xs, dim=1), L_reg) if self.training else F.log_softmax(xs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "num_points = 128\n",
    "save_name = \"PointNet.pt\"\n",
    "batch_size = 512\n",
    "\n",
    "########### loading data ###########\n",
    "\n",
    "train_data = ModelNet40(num_points)\n",
    "test_data = ModelNet40(num_points, 'test')\n",
    "\n",
    "train_size = int(0.9 * len(train_data))\n",
    "valid_size = len(train_data) - train_size\n",
    "train_data, valid_data = Data.random_split(train_data, [train_size, valid_size])\n",
    "valid_data.partition = 'valid'\n",
    "train_data.partition = 'train'\n",
    "\n",
    "print(\"train data size: \", len(train_data))\n",
    "print(\"valid data size: \", len(valid_data))\n",
    "print(\"test data size: \", len(test_data))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    Xs = torch.stack([X for X, _ in batch])\n",
    "    Ys = torch.tensor([Y for _, Y in batch], dtype = torch.long)\n",
    "    return Xs, Ys\n",
    "\n",
    "train_iter  = Data.DataLoader(train_data, shuffle = True, batch_size = batch_size, collate_fn = collate_fn)\n",
    "valid_iter = Data.DataLoader(valid_data, batch_size = batch_size, collate_fn = collate_fn)\n",
    "test_iter = Data.DataLoader(test_data, batch_size = batch_size, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### loading model ####################\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = PointNet(nfeat=3, nclass=40, dropout=0.3)\n",
    "net.to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### training #########################\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr, weight_decay = 0.0001)\n",
    "loss = nn.NLLLoss()\n",
    "\n",
    "def adjust_lr(optimizer, decay_rate=0.95):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= decay_rate\n",
    "\n",
    "retrain = True\n",
    "if os.path.exists(save_name):\n",
    "    print(\"Model parameters have already been trained before. Retrain ? (y/n)\")\n",
    "    ans = input()\n",
    "    if (ans == 'y'):\n",
    "        checkpoint = torch.load(save_name, map_location = device)\n",
    "        net.load_state_dict(checkpoint[\"net\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "\n",
    "train_model(train_iter, valid_iter, net, loss, optimizer, device = device, max_epochs = int(1000/(batch_size/64)), \n",
    "            adjust_lr = adjust_lr, early_stop = EarlyStop(patience = 20, save_name = save_name))\n",
    "    \n",
    "\n",
    "############### testing ##########################\n",
    "\n",
    "loss, acc = evaluate_model(test_iter, net, loss)\n",
    "print('test acc = %.6f' % (acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}