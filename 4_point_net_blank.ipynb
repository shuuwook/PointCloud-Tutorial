{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "from module.dataset import ModelNet40\n",
    "from module.utils import *\n",
    "\n",
    "import os, sys\n",
    "from collections import OrderedDict\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "init 함수에서 feature_dim을 input으로 받아 Batch Normalization을 수행합니다.\n",
    "BatchNorm(3), BatchNorm(64), BatchNorm(1024)와 같이 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    '''\n",
    "        Perform batch normalization.\n",
    "        Input: A tensor of size (N, M, feature_dim), or (N, feature_dim, M) (available when feature_dim != M), \n",
    "                or (N, feature_dim)\n",
    "        Output: A tensor of the same size as input.\n",
    "    '''\n",
    "    def __init__(self, feature_dim):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.batchnorm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "init 함수에서 받은 tuple형태의 param 파라미터를 통해 torch.permute_(param)을 실행합니다.\n",
    "예를 들어, x가 (100, 200, 300)의 shape를 갖고 있을 때 x.permute((0, 1, 2))는 x와 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Permute(nn.Module):\n",
    "    def __init__(self, param):\n",
    "        super(Permute, self).__init__()\n",
    "        self.param = param\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.permute(self.param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully connected layers로 이루어진 MLP를 구성합니다.\n",
    "일반적으로 Fully connected layer, Batch normalization, Activation function이 set로 구성됩니다.\n",
    "\n",
    "init 함수에서 hidden_size 파라미터는 \n",
    "input dimension부터 hidden dimension들, output dimension까지의 tuple로 입력받습니다.\n",
    "\n",
    "input dimension부터 output dimension까지를 한번에 입력받는다는 점에 주의해주세요.\n",
    "\n",
    "batchnorm과 last_activation argument에 따라 옵션을 줄 수 있도록 작성하면 좋지만, 무시하셔도 괜찮습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, batchnorm = True, last_activation = True):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적인 max pooling이 아닌 global maxpooling입니다.\n",
    "input의 shape가 (B, N, D)일 때, output의 shape가 (B,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaxPooling, self).__init__()\n",
    "\n",
    "    def forward(self, x, dim=1, keepdim = False):\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input 또는 중간의 feature를 permutation과 rigid motion에 invariant하도록 만들기 위한 모듈입니다.\n",
    "- nfeat, 64, 128, 1024로 mapping되는 MLP\n",
    "- max pooling, batch normalization\n",
    "- 다시 1024, 512, 256, nfeat*nfeat로 mapping되는 MLP로 구성됩니다.\n",
    "\n",
    "최종적으로 (B, n_feat*n_feat)의 output을 (B, n_feat, n_feat)로 shape을 변경해 return해주면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNet(nn.Module):\n",
    "    def __init__(self, nfeat):\n",
    "        super(TNet, self).__init__()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        return self.tnet(x).view(batch_size, self.nfeat, self.nfeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 모듈들을 모두 조합해 최종적으로 point cloud classification을 위한 pointNet을 구성해보겠습니다.\n",
    "TNet의 output은 transform matrix이므로 TNet의 input과 matrix multiplication을 해야합니다.\n",
    "\n",
    "- TNet : (B,N,3) > (B,N,3)\n",
    "- BatchNorm : (B,N,3) > (B,N,3)\n",
    "- MLP : (B,N,3) > (B,N,64) > (B,N,64)\n",
    "- TNet : (B,N,64) > (B,N,64)\n",
    "- BatchNorm : (B,N,64) > (B,N,64)\n",
    "- MLP : (B,N,64) > (B,N,64) > (B,N,128) > (B,N,1024)\n",
    "- Maxpooling : (B,N,1024) > (B,1024)\n",
    "- BatchNorm : (B,1024) > (B,1024)\n",
    "- MLP : (B,1024) > (B,512) > (B,256)\n",
    "- Dropout : (B,256) > (B,256)\n",
    "- FC-layer (Linear or Conv1d) for k classes : (B,256) > (B,nclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNet(nn.Module):\n",
    "    def __init__(self, nfeat, nclass, dropout = 0):\n",
    "        super(PointNet, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.eye64 = torch.eye(64).to(device)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        batch_size = xs.shape[0]\n",
    "        \n",
    "        \n",
    "        if (self.training):\n",
    "            transform_transpose = transform.transpose(1, 2)\n",
    "            tmp = torch.stack([torch.mm(transform[i], transform_transpose[i]) for i in range(batch_size)])\n",
    "            L_reg = ((tmp - self.eye64) ** 2).sum() / batch_size\n",
    "            \n",
    "        return (F.log_softmax(xs, dim=1), L_reg) if self.training else F.log_softmax(xs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Arguments ###################\n",
    "\n",
    "lr = 0.001\n",
    "num_points = 128\n",
    "save_name = \"PointNet.pt\"\n",
    "batch_size = 512\n",
    "\n",
    "################## loading data ##################\n",
    "\n",
    "train_data = ModelNet40(num_points)\n",
    "test_data = ModelNet40(num_points, 'test')\n",
    "\n",
    "train_size = int(0.9 * len(train_data))\n",
    "valid_size = len(train_data) - train_size\n",
    "train_data, valid_data = Data.random_split(train_data, [train_size, valid_size])\n",
    "valid_data.partition = 'valid'\n",
    "train_data.partition = 'train'\n",
    "\n",
    "print(\"train data size: \", len(train_data))\n",
    "print(\"valid data size: \", len(valid_data))\n",
    "print(\"test data size: \", len(test_data))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    Xs = torch.stack([X for X, _ in batch])\n",
    "    Ys = torch.tensor([Y for _, Y in batch], dtype = torch.long)\n",
    "    return Xs, Ys\n",
    "\n",
    "train_iter  = Data.DataLoader(train_data, shuffle = True, batch_size = batch_size, collate_fn = collate_fn)\n",
    "valid_iter = Data.DataLoader(valid_data, batch_size = batch_size, collate_fn = collate_fn)\n",
    "test_iter = Data.DataLoader(test_data, batch_size = batch_size, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### loading model ####################\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = PointNet(nfeat=3, nclass=40, dropout=0.3)\n",
    "net.to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### training #########################\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr, weight_decay = 0.0001)\n",
    "loss = nn.NLLLoss()\n",
    "\n",
    "def adjust_lr(optimizer, decay_rate=0.95):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= decay_rate\n",
    "\n",
    "retrain = True\n",
    "if os.path.exists(save_name):\n",
    "    print(\"Model parameters have already been trained before. Retrain ? (y/n)\")\n",
    "    ans = input()\n",
    "    if (ans == 'y'):\n",
    "        checkpoint = torch.load(save_name, map_location = device)\n",
    "        net.load_state_dict(checkpoint[\"net\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "\n",
    "train_model(train_iter, valid_iter, net, loss, optimizer, device = device, max_epochs = int(1000/(batch_size/64)), \n",
    "            adjust_lr = adjust_lr, early_stop = EarlyStop(patience = 20, save_name = save_name))\n",
    "    \n",
    "\n",
    "############### testing ##########################\n",
    "\n",
    "loss, acc = evaluate_model(test_iter, net, loss)\n",
    "print('test acc = %.6f' % (acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pointnet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "8252d3e94ebb591f41c605bc25f3dfaf90186a33ae2e5a5cd565ddbc7c55c9a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
